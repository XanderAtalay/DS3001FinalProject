---
title: "Movie Metrics and Models"
author: "Xander Atalay, Zoe Pham, Umar Abushaban"
date: "12/8/2021"
output:
  html_document:
    toc: yes
    theme: cosmo
    toc_float: yes
  pdf_document:
    toc: yes
editor_options:
  chunk_output_type: inline
---
<center><img src="https://broomearts.org/wp-content/uploads/2019/03/filmreel.jpg"></center>

## Introduction

### Background and Question
  Our group is taking a look at [The Movie Dataset](https://www.kaggle.com/rounakbanik/the-movies-dataset/metadata) from Kaggle.com. This dataset contains "Metadata on over 45,000 movies. 26 million ratings from over 270,000 users." The metadata for movies include their runtime, genres, ratings, revenue, budget, and more. We're going to be exploring this data and the relationships within it, with the primary goal of creating models that give us insight into what makes a move successful, including the ability to predict the success of a movie.
  
  We can define success depending on our buisness metric of intrest. This buisness metric depends on what we, as a production company or movie studio, are looking to accomplish. If we want to create award winning movies, we should focus on reviews. If we want to make a lot of money, we can shift our focus to revenue. How we classify movies will be dependent on the metric we're looking at and the models we make. Of course, the key to a good movie, as described by filmdraft.com, is the combination of story, characters, actors, directors, and post-production - any of which could make or break a film. That said, it would be interesting to know whether or not a space western is less likely to succeed critically than a drama, so we can still make those general associations with this data.
  
### Cleaning the data

  The dataset that we're starting out with is not ready for a deep analysis, and you can see the steps I took to clean it in the FeatureEngineering.rmd in our project repository. Essentially, the genres parameter was formatted such that we wouldn't be able to use it as a factor, as it was originally a character list of the different genres within a movie and, there being 24 genres, the number of combinations was immense. Thus, we instead one-hot encoded each genre, which gives us greater flexibility with our exploratory data analysis and will make our models far more efficient.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, cache = TRUE)
```

```{r Libraries, include=FALSE}
library(tidyverse)
library(ggplot2)
library(e1071)
library(htmltools)
library(devtools)
library(NbClust)
library(class)
library(plotly)
library(rio)
library(plyr)
library(rpart)
library(psych)
library(pROC)
library(rpart.plot)
library(rattle)
library(caret)
library(C50) 
library(mlbench)
library(randomForest)
library(MLmetrics)
library(ROCR)
library(mltools)
library(data.table)
library("ggpubr")
theme_set(
  theme_bw() +
    theme(legend.position = "top")
  )
```

## Exploratory Data Analysis {.tabset}

I have created different tabs to organize a data exploration approach. In "The Dataset", you can read about the structure of the feature engineered dataset that we'll be using for all of our models. In "Parameters of Interest", I added additional tabs to look more in-depth with the parameters that I think are most important to our dataset, including the parameters that we're using to quantify success (revenue and vote_average). In "Evaluating Relationships", I make linear models that assess the relationships between our success parameters and our features, which won't be used to actually determine the success of movies but will instead give us an idea of the significant relationships in our data so that we have an idea of what to optimize in our models. Finally, I'm interested in seeing if movies of specific genres tend to cluster together, so I'll be making a rough clustering model just to give us a better understanding of these data.

```{r Loading in Data}
Movies <- read.csv("./Movies.csv")
Titles <- read.csv("./Titles.csv")
```

### HIDE
### The Dataset

Once again, we're getting this data from [The Movie Dataset](https://www.kaggle.com/rounakbanik/the-movies-dataset/metadata) from Kaggle.com. After feature engineering our data, this is the structure of our dataset: 
```{r Exploring Data I}
str(Titles)
```
For the models themselves, we'll make sure to remove the movie titles and parameters that we're looking at. Otherwise, it's clear that a majority of columns in this data frame are genre columns, so I hope that they turn out to be good predictors of our success metrics. Another success metric we could have looked at is popularity, but I think that most studios are more interested in making money and having critical acclaim than just getting views, so we're only going to look at those two metrics for now.

### Parameters of Interest {.tabset}

#### Relative success
Let's start by analyzing the metrics we're using to determine whether a movie is good or not, which are revenue and vote_average. Again, the metric we use for our model depends on our aims as a studio. If we want to make money, we'll optimize our model to predict revenue. If we want to make art, we'll go for vote_average instead.

Here are some histograms for revenue and vote_average:
```{r Revenue Plot I}
(ggplot(Movies, aes(x = (revenue))) 
  + geom_histogram(bins = 50)
  + geom_histogram(bins = 50, color = "black", fill = cm.colors(50, alpha = 0.75))) 
```

The original revenue histogram doesn't really show us much because, as expected, the amount of money that a movie can make has a huge range with only few reaching the billion dollar mark. There also appears to be a lot of missing data. Therefore, it might be more helpful to visualize the parameter in log form, which will bring all of our values to a more similar range.

```{r Revenue Plot II}
Movies <- Movies[Movies$revenue != 0,]
Movies <- Movies[Movies$vote_average != 0,]

Titles <- Titles[Titles$revenue != 0,]
Titles <- Titles[Titles$vote_average != 0,]

(ggplot(Movies, aes(x = log(revenue))) 
  + geom_histogram(bins = 50)
  + geom_histogram(bins = 50, color = "black", fill = cm.colors(50))) 
```
When we look at the log of revenue, we can see that there is a much better distribution, but we lose a lot of data because of all of those zeros. Regardless, we should have enough here still to make a strong model.

Next, let's take a look at vote_average. Here is a histogram of the average vote a movie received, again taking out any missing values. I'll also include a 5 number summary because the numbers are easier to interpret than revenue:

```{r vote_average hist}
(ggplot(Movies, aes(x = vote_average)) 
  + geom_histogram(bins = 50, color = "black", fill = cm.colors(50))) 

summary(Movies$vote_average)
```
This is a pretty interesting distribution, and I think it's particularly funny that the average rating is 6.25 (as opposed to 5).

Finally, we can make a linear regression model and create a plot to explore how these two metrics of success are related, and make sure that they are not just proxies for one another.

```{r vote_average vs revenue}
(ggplot(Movies, aes(x = vote_average, y = log(Movies$revenue), alpha = revenue)) + 
   geom_point(size = 0.5, alpha = 0.5) + 
   stat_smooth(method = "lm", color = "maroon") + 
   xlab("Rating") + 
   ylab("Revenue"))

summary(lm(Movies$revenue ~ Movies$vote_average))
```

As expected, we see a significant relationship between these two parameters, but the R squared is small enough that they are still distinct enough to try to optimize seperately. With that, let's take a look at some of the other metrics in our dataset that will help us build powerful predictive models.



#### Genres
This is one of the more interesting metrics we have acess to, and I'm excited to start looking into what the most popular genres / genre combinations are. To start, let's just take a look at the relative proportion of each genre in this dataset. Keep in mind that a movie can have more than one genre. Here is a 
```{r Genre DF}

genres <- colnames(Movies)[8:24]
counts <- c()
gmean <- c()
grevMean <- c()
gsd <- c()
grevSD <- c()
for(val in genres){
  genreDF <- Movies[Movies[,val] == TRUE,]
  genreNum <- length(genreDF$budget)
  counts <- c(counts, genreNum)
  gmean <- c(gmean, mean(genreDF$vote_average))
  gsd <- c(gsd, sd(genreDF$vote_average))
  grevMean <- c(grevMean, mean(genreDF$revenue))
  grevSD <- c(grevSD, sd(genreDF$revenue))
}
genres <- data.frame(genres, counts, gmean, gsd, grevMean, grevSD)

```

```{r Genre Frequency}

(ggplot(genres, aes(x = genres, y = counts, fill = counts)) + 
   geom_bar(stat = "identity") + 
   scale_color_continuous() + 
   theme(axis.text.x = element_text(angle = -90)) + 
   xlab("Genre") + 
   ylab("Frequency"))

```
As you can see, the most movies we have of any genre we have in this dataset is dramas, which makes some sense because it's a genre that can be combined with almost any other. Now that we have a sense of how popular each genre is, let's take a look at if the type of genre a move is has any effect on its rating.

Here is a plot of the genres and their average rating:

```{r Genre Ratings}
(ggplot(genres, aes(x = genres, y = gmean, fill = gmean)) 
  + geom_bar(stat="identity", color="black", position=position_dodge()) 
  + geom_errorbar(aes(ymin=gmean-gsd, ymax=gmean+gsd), width=.2, position=position_dodge(.9)) 
  + theme(axis.text.x = element_text(angle = -90)) + 
   xlab("Genre") + 
   ylab("Average Rating"))
```

It doesn't look like there is a ton of significance, but we can still test each genre individually as having a relationship with average vote. For example, here is a lm of the horror genre:

```{r Genre Horror LM}
summary(lm(Movies$vote_average ~ Movies$Horror))
```
Interestingly, we do see significance, which means that this genre will help our model determine whether or not a movie will have good votes. Now, let's see how the average revenue of each genre compares to one another.

```{r Genre Revenue}
(ggplot(genres, aes(x = genres, y = grevMean, fill = grevMean)) 
  + geom_bar(stat="identity", color="black", position=position_dodge()) 
  + theme(axis.text.x = element_text(angle = -90)) + 
   xlab("Genre") + 
   ylab("Revenue"))
```
We see some more interesting data here, and some of it can be expected. Genres like action, adventure, fantasy, and science fiction are generally the genres that we see in blockbuster movies, or movies that generate a huge amount of revenue. Documentaries, on the other hand, normally aren't even released in theaters, so we would expect much less revenue from them.


#### Budget
Finally, let's take a look at the budget data we have, which I expect to be a big predictor of average vote and even more so revenue.

```{r Budget Plot I}
(ggplot(Movies, aes(x = (budget))) 
  + geom_histogram(bins = 50)
  + geom_histogram(bins = 50, color = "black", fill = cm.colors(50, alpha = 0.75))) 
```

As you can see, we've already standardized the variable. Just like revenue, however, we can see the enormous range that this metric has, which makes looking at the log useful.

```{r Budget Plot II}
(ggplot(Movies, aes(x = log(budget))) 
  + geom_histogram(bins = 50)
  + geom_histogram(bins = 50, color = "black", fill = cm.colors(50))) 
```
Finally, lets take a look at the relationship between budget and our two metrics of success.
```{r vote_average vs budget}
summary(lm(Movies$vote_average ~ Movies$budget))

summary(lm(Movies$revenue ~ Movies$budget))
```
Interestingly enough, we don't see a significant relationship between budget and vote_average, but as expected, there is a strong relationship between budget and revenue. This means that the models we build that optimize for revenue will rely on the budget variable much more than the models that optimize average vote.


### Clustering
As the final part of our exploratory data analysis, I'm interested in seeing if our data clusters into the four most popular genres: Action, Drama, Comedy, and Thriller. I'll start by including only data where at least one of those things is true.
```{r}
clusterGenres <- Movies[(Movies$Action | Movies$Comedy | Movies$Drama | Movies$Thriller),]
Movie_drop <- c("vote_average", "revenue", "original_language", "Action", "Drama", "Comedy", "Thriller")
Movie_Cluster_Data <- clusterGenres[, !(names(clusterGenres)) %in% Movie_drop]

set.seed(1)
kmeans_Movies = kmeans(Movie_Cluster_Data, centers = 4, algorithm = "Lloyd")
Movie_Clusters = as.factor(kmeans_Movies$cluster)

Action <- (ggplot(clusterGenres, aes(y = revenue, 
                x = vote_average,
                shape = Movie_Clusters,
                color = Action)) + 
  geom_point(size = 1) +
  scale_shape_manual(name = "Cluster", 
                     labels = c("Cluster 1", "Cluster 2", "Cluster 3", "Cluster 4"),
                     values = c("circle open", "circle", "square open", "square filled")) +
  theme_light())

Drama <- (ggplot(clusterGenres, aes(y = revenue, 
                x = vote_average,
                shape = Movie_Clusters,
                color = Drama)) + 
  geom_point(size = 1) +
  scale_shape_manual(name = "Cluster", 
                     labels = c("Cluster 1", "Cluster 2", "Cluster 3", "Cluster 4"),
                     values = c("circle open", "circle", "square open", "square filled")) +
  theme_light())

Comedy <- (ggplot(clusterGenres, aes(y = revenue, 
                x = vote_average,
                shape = Movie_Clusters,
                color = Comedy)) + 
  geom_point(size = 1) +
  scale_shape_manual(name = "Cluster", 
                     labels = c("Cluster 1", "Cluster 2", "Cluster 3", "Cluster 4"),
                     values = c("circle open", "circle", "square open", "square filled")) +
  theme_light())

Thriller <- (ggplot(clusterGenres, aes(y = revenue, 
                x = vote_average,
                shape = Movie_Clusters,
                color = Thriller)) + 
  geom_point(size = 1) +
  scale_shape_manual(name = "Cluster", 
                     labels = c("Cluster 1", "Cluster 2", "Cluster 3", "Cluster 4"),
                     values = c("circle open", "circle", "square open", "square filled")) +
  theme_light())

Clusterfigure <- ggarrange(Action, Drama, Comedy, Thriller,
                    labels = c("Action", "Drama", "Comedy","Thriller"),
                    ncol = 2, nrow = 2)

Clusterfigure

```

Unfortunately, this figure becomes a bit garbled and difficult to analyze. That said, we can definitely see that each of these genres occupies a different space on the plot of average vote vs revenue. Action movies tend to have higher revenue and average ratings with dramas being rated higher but not making as much money. Comedy and Thrillers are more spread out, it appears that comedy doesn't make much money or get high votes, which makes sense as they're mostly made for entertainment factor instead of being huge hits or award winning films.


## KNN for Average Vote {.tabset}

### Hide

### Preparing Data
```{r Preparing Data, echo=TRUE}
Movies <- read.csv('./Movies.csv')
str(Movies) 
Movies$vote_average
# Deleting rows where vote_average = 0 as a flaw in the data
Movies <- Movies[Movies$vote_average !=0,]
# Converting rating into a factor, with level 1 greater than 7.5 and 0 otherwise.
Movies$Rating <- 0
Movies[Movies$vote_average > 6.5,]$Rating <- 1
Movies[Movies$vote_average <= 6.5,]$Rating <- 0
Movies <- Movies[,-6]
str(Movies)
```


### Data Partitioning                
```{r Zoe Data Partitioning}
# Determining data composition and baseline/prevalence
(table(Movies$Rating)[2])/(sum(table(Movies$Rating)))
Movies$Rating
# Partition into train, tune, and test
part_index_1 <- createDataPartition(Movies$Rating,
                                           times=1,
                                           p = 0.70,
                                           groups=1,
                                           list=FALSE)
train <- Movies[part_index_1,]
test <- Movies[-part_index_1, ]
dim(train)
dim(test)
```
This prevalence of 34% shows the random chance of determining a highly rated movie, as reflected by the actual prevalence of highly rated movies (above a 6.5 vote_average).

### Selecting "k"
```{r Selecting K}
# Function to calculate classification accuracy based on the number of "k."
chooseK = function(k, train_set, val_set, train_class, val_class){
  
  # Build knn with k neighbors considered.
  set.seed(3001)
  class_knn = knn(train = train_set,    #<- training set cases
                  test = val_set,       #<- test set cases
                  cl = train_class,     #<- category for classification
                  k = k,                #<- number of neighbors considered
                  use.all = TRUE)       #<- control ties between class assignments
                                        #   If true, all distances equal to the kth 
                                        #   largest are included
  conf_mat = table(class_knn, val_class)
  
  # Calculate the accuracy.
  accu = sum(conf_mat[row(conf_mat) == col(conf_mat)]) / sum(conf_mat)                         
  cbind(k = k, accuracy = accu)
}
set.seed(3001)
knn_different_k = sapply(seq(1, 21, by = 2),  #<- set k to be odd number from 1 to 21
                         function(x) chooseK(x, 
                          train_set = train[, c("runtime", "budget", "Animation", "Comedy", "Adventure", "Fantasy", "Drama", "Romance", "Action", "Crime", "Thriller", "History", "ScienceFiction", "Mystery", "Western", "Horror", "Documentary", "Music", "War")],
                          val_set = test[, c("runtime", "budget", "Animation", "Comedy", "Adventure", "Fantasy", "Drama", "Romance", "Action", "Crime", "Thriller", "History", "ScienceFiction", "Mystery", "Western", "Horror", "Documentary", "Music", "War")],
                          train_class = train$Rating,
                          val_class = test$Rating))
class(knn_different_k)#matrix 
head(knn_different_k)
knn_different_k = data.frame(k = knn_different_k[1,],
                             accuracy = knn_different_k[2,])
# Plot accuracy vs. k.
k_plot <- ggplot(knn_different_k,aes(x = k, y = accuracy)) +
  geom_line(color = "orange", size = 1.5) +
  geom_point(size = 3) +
  ggtitle("KNN Elbow Chart")
k_plot
```
A KNN Elbow Chart was created above plotting k against accuracy to determine that 3 nearest neighbors seems to be the best choice for k because the model's accuracy peaks at 91% when k = 3 before trailing off as k increases.


### Training the K Classifier
```{r Training the Classifier}
# Training the k classifier using the class package. 
# Setting seed so results are reproducible from KNN's randomized algorithm
set.seed(3001)
# Target variables include runtime, budget, and genre. Revenue and popularity were excluded as these are highly correlated to rating, and are only known after the movie is released, which isn't helpful for our business question of movie production.
movies_3NN <-  knn(train = train[, c("runtime", "budget", "Animation", "Comedy", "Adventure", "Fantasy", "Drama", "Romance", "Action", "Crime", "Thriller", "History", "ScienceFiction", "Mystery", "Western", "Horror", "Documentary", "Music", "War")],#<- training set cases
               test = test[, c("runtime", "budget", "Animation", "Comedy", "Adventure", "Fantasy", "Drama", "Romance", "Action", "Crime", "Thriller", "History", "ScienceFiction", "Mystery", "Western", "Horror", "Documentary", "Music", "War")],    #<- test set cases
               cl = train$Rating,#<- category for true classification
               k = 3,#<- number of neighbors considered
               use.all = TRUE,
               prob = TRUE) #<- control ties between class assignments If true, all distances equal to the kth largest are included
# Viewing the output:
str(movies_3NN)
table(movies_3NN)
length(movies_3NN)
```

### KNN Classification Comparison
```{r KNN Classification Comparison}
# How does the kNN classification compare to the true class?
# Combining the predictions from movies_3NN to the original data set.
kNN_res = table(movies_3NN,
                test$Rating)
kNN_res
sum(kNN_res)  
# TP TN
kNN_res[row(kNN_res) == col(kNN_res)]
# Calculate the accuracy rate by dividing the correct classifications by the total number of classifications.
kNN_acc = sum(kNN_res[row(kNN_res) == col(kNN_res)]) / sum(kNN_res)
kNN_acc
# Our KNN model returns a 90 % accuracy rate of predicting a highly rated move (with an average vote above 7.5), which is a great improvement from the baserate of 8.1%.
str(movies_3NN)
str(as.factor((test$Rating)))
confusionMatrix(as.factor(movies_3NN), as.factor(test$Rating), positive = "1", dnn=c("Prediction", "Actual"), mode = "sens_spec")
      
#Reference for confusion matrix: https://www.rdocumentation.org/packages/caret/versions/6.0-86/topics/confusionMatrix 
```
Our KNN model confusion matrix produces a 65 % accuracy rate of predicting a highly rated move (a movie with an average vote above 7.5), which is a great improvement from the baserate of 8.1%. The sensitivity or true positive rate, when highly rated movies are accurately classified, is 35%, which is very low. However, the model's specificity is 81%, and false positive rate is 100% - 81% = 19%, which is very low, meaning the model rarely inaccurately classifies poorly rated movies as highly rated ones. 

The high accuracy and specificity show our model usually accurately classifies movie ratings, which is important in informing our business problem of deciding how to create a highly rated movie based on genre, budget, and runtime.



## DT for Revenue {.tabset}

### Hide

The goal here is really to be able to create a model that can predict if a movie is going to generate at least $5 million in revenue. To do this, we're going to use C5.0, which will construct decision trees in two phases: First, it will generate a larger tree to loosely fit the data, and then it will be 'pruned' down to something more tightly fitting.

### Pre-processing:
```{r RevDT Pre-Processing I}
movies = read.csv('./Movies.csv')
length(movies[movies$revenue == 0, ]$budget) #most of the movies dont have any revenue
movies[movies == "0"] <- NA
movies[movies == "?"] <- NA
movies <- movies[complete.cases(movies),]
na.omit(movies)
#movies = movies[1:1000,]
#View(movies)
#Lets add a new column to the dataset that turns movies that made less than $5,000,000 revenue into 0s, and 
#movies that made more than $5,000,000 into 1s. This is what we want our model to be able to classify.
movies = movies %>%
  mutate(new_rev = case_when(revenue > 5000000 ~ "moreThan5Mil",
                             revenue < 5000000 ~ "LessThan5Mil"
                             #TRUE ~ 0 #check this
                             ))
```

```{r RevDT Pre-Processing II}
#Change to factors
movies[2] = as.factor(movies$original_language)
movies[25] = as.factor(movies$new_rev)
movies[,c(8:24)] = lapply(movies[,c(8:24)], as.factor)
movies$original_language <- fct_collapse(movies$original_language,
                        English = "English",
                        Other = c("Italian","French","German","Japanese")
                        )
```

### Partitioning
```{r RevDT Partitioning}
#There is not a easy way to create 3 partitions using the createDataPartitions
#so we are going to use it twice. Mostly because we want to stratify on the variable we are working to predict. 
part_index_1 <- caret::createDataPartition(movies$new_rev,
                                           times=1,
                                           p = 0.70,
                                           groups=1,
                                           list=FALSE)
train <- movies[part_index_1, ]
tune_and_test <- movies[-part_index_1, ]
#The we need to use the function again to create the tuning set 
tune_and_test_index <- createDataPartition(tune_and_test$new_rev,
                                           p = .5,
                                           list = FALSE,
                                           times = 1)
tune <- tune_and_test[tune_and_test_index, ]
test <- tune_and_test[-tune_and_test_index, ]
dim(train)
dim(test)# these will be slightly off because the data set isn't perfectly even
#buts its not a issue. 
dim(tune)
```

```{r RevDT Features}
# Choose the features and classes
features <- train[,c(-4,-25)] #dropping 4 and 25. 4 is the original revenue count, which could be used to perfectly predict the 0 or 1 value, which is column 25
summary(movies$revenue)
```

Baseline
```{r}
table(movies$new_rev)[2]/sum(table(movies$new_rev))
```

### Initial Model
```{r RevDT Generating Model}
target <- train$new_rev
str(features)
str(target)
#Cross validation process 
fitControl <- trainControl(method = "repeatedcv",
                          number = 10,
                          repeats = 5, 
                          returnResamp="all",
                          classProbs = TRUE,
                          allowParallel = TRUE) 
# number - number of folds
# repeats - number of times the CV is repeated, here it's 5 take the average of
# those 5 repeats
# Grid search options for each of the models available in CARET
# http://topepo.github.io/caret/train-models-by-tag.html#tree-based-model
grid <- expand.grid(.winnow = c(TRUE,FALSE), 
                    .trials=c(1,5,10,15,20), 
                    .model="tree")
#expand.grid - series of options that are available for model training
#winnow - whether to reduce the feature space -  Works to remove unimportant 
#features but it doesn't always work, in the above we are winnowing.  
#trails - number of boosting iterations to try, 1 indicates a single model 
#model - type of ml model
set.seed(1984)
movies_mdl <- train(x=features,
                y=target,
                method="C5.0",
                tuneGrid=grid,
                trControl=fitControl,
                verbose=TRUE)
movies_mdl #provides us the hyper-parameters that were selected through the grid
# search process. 
# visualize the re-sample distributions
xyplot(movies_mdl,type = c("g", "p", "smooth"))
varImp(movies_mdl)
```

### Model Eval
Let's use the model to predict and the evaluate the performance
```{r RevDT Model Eval I}
movies_pred_tune = predict(movies_mdl,tune, type= "raw")
#View(as_tibble(movies_pred_tune))
#Lets use the confusion matrix
(movies_eval <- confusionMatrix(as.factor(movies_pred_tune), 
                as.factor(tune$new_rev), 
                dnn=c("Prediction", "Actual"), 
                mode = "sens_spec"))
#table(tune$new_rev)
(movies_pred_tune_p = predict(movies_mdl,tune,type= "prob"))
```

### Tuning

Now we can optimize the model using the tune dataset. We can create a few models and see which ones are the most optimal in terms of their metrics, such as accuracy and sensitivity.

Let's make some changes and see if we can improve

```{r RevDT Optimization I}
#Cross Validation Process, changing method for CV and adding a different metric for optimization 
library(MLmetrics)
f1 <- function(data, lev = NULL, model = NULL) {
  f1_val <- F1_Score(y_pred = data$pred, y_true = data$obs, positive = lev[1])
  c(F1 = f1_val)
}
#source: https://stackoverflow.com/questions/37666516/caret-package-custom-metric
fitControl_2 <- trainControl(method = "LGOCV",
                          number = 10, 
                          returnResamp="all",
                          classProbs = TRUE,
                          allowParallel = TRUE,
                          summaryFunction = f1) 
# grid search, increasing the boosting rounds 
grid_2 <- expand.grid(.winnow = c(TRUE,FALSE), 
                    .trials=c(10,15,20,25,30), 
                    .model="tree")
# training model
set.seed(1984)
movies_mdl_2 <- train(x=features,
                y=target,
                method="C5.0",
                tuneGrid=grid_2,
                metric="F1",
                trControl=fitControl_2)
movies_mdl
movies_mdl_2
```

Model 2 Eval:

```{r RevDT Model Eval II}
movies_pred_tune_2 = predict(movies_mdl_2,tune, type= "raw")
#Lets use the confusion matrix
(model_eval_2 <- confusionMatrix(as.factor(movies_pred_tune_2), 
                as.factor(tune$new_rev), 
                dnn=c("Prediction", "Actual"), 
                mode = "sens_spec"))
model_eval_2
movies_eval
```

Now give the changes we made above let's final the model and check the metrics output on the test file. 

### Final Model
```{r Rev DT Model Eval III}
#This is actually pretty good. Increased the sensitivity by more than 10% without losing much accuracy
movies_pred_test = predict(movies_mdl_2,test, type= "raw")
#Using the confusion matrix:
confusionMatrix(as.factor(movies_pred_test), 
                as.factor(test$new_rev), 
                dnn=c("Prediction", "Actual"), 
                mode = "sens_spec")
(movies_pred_tune_p = predict(movies_mdl,test,type= "prob"))
```

It seems that the final model is decent. The accuracy stayed relatively similar, but overall as we progressed through these models, the sentivitiy got a lot higher. 94% sensitivity.

 




## RF for Revenue {.tabset}

### Hide

### Pre-processing
```{r RevRF Preprocessing}
movies = read.csv('./Movies.csv')
movies[movies == "0"] <- NA
movies[movies == "?"] <- NA
movies <- movies[complete.cases(movies),]
na.omit(movies)
#movies = movies[1:1000,]
movies = movies %>%
  mutate(new_rev = case_when(revenue > 1000000 ~ ">1mil",
                             revenue < 1000000 ~ "<1mil"
                             #TRUE ~ 0 #check this
                             ))
#turn all the character columns into factor.
movies[2] = as.factor(movies$original_language)
movies[25] = as.factor(movies$new_rev)
#boolean to factor:
movies[,c(8:24)] = lapply(movies[,c(8:24)], as.factor)
movies = movies[,-c(6)]
```
Baseline
```{r}
table(movies$new_rev)[2]/sum(table(movies$new_rev))
#baseline of 90%. Lets see if our model has a higher accuracy than this
```

### Partitioning
Create test, tune and training sets 
```{r RevRF Partitioning}
#Split your data into test, tune, and train. (70/15/15)
sample_rows = 1:nrow(movies)
#sample_rows
set.seed(1984) #sample(x, size, replace = FALSE, prob = NULL)
test_rows = sample(sample_rows,
                   dim(movies)[1]*.10, #start with 10% of our dataset, could do 20%
                   # but random forest does require more training data because of the 
                   # sampling so 90% might be a better approach with this small of a dataset
                   replace = FALSE)# We don't want duplicate samples
str(test_rows)
#movies_train will hold 90% of the data, we're gonna use this to create different random forest models, but that other 10% that I wont touch will be in the movies_test version. Going to use that later to evaluate the model. Will be independent and untouched while I play around with the random forest models in the beginning.
movies_train = movies[-test_rows,]
movies_test = movies[test_rows,]
#Splitting the Data
#There is not a easy way to create 3 partitions using the createDataPartitions
#so we are going to use it twice. Mostly because we want to stratify on the variable we are working to predict. What does that mean?  
part_index_1 <- caret::createDataPartition(movies$new_rev,
                                           times=1,
                                           p = 0.70,
                                           groups=1,
                                           list=FALSE)
train <- movies[part_index_1, ]
tune_and_test <- movies[-part_index_1, ]
#The we need to use the function again to create the tuning set 
tune_and_test_index <- createDataPartition(tune_and_test$new_rev,
                                           p = .5,
                                           list = FALSE,
                                           times = 1)
tune <- tune_and_test[tune_and_test_index, ]
test <- tune_and_test[-tune_and_test_index, ]
dim(train)
dim(test)# these will be slightly off because the data set isn't perfectly even
#buts its not a issue. 
dim(tune)
```

Calculate the initial mtry level 
```{r RevRF mytry}
mytry_tune <- function(x){
  xx <- dim(x)[2]-1
  sqrt(xx)
}
       
mytry_tune(movies)
```

### Initial Model
Run the initial RF model with 500 trees 
```{r RevRF Model I, results='hide'}
#default to 500
set.seed(2023)	
movies_RF = randomForest(new_rev~., na.action=na.omit,  #<- Formula: response variable ~ predictors.
                            #   The period means 'use all other variables in the data'.
                            movies_train,     #<- A data frame with the variables to be used.
                            #y = NULL,           #<- A response vector. This is unnecessary because we're specifying a response formula.
                            #subset = NULL,      #<- This is unnecessary because we're using all the rows in the training data set.
                            #xtest = NULL,       #<- This is already defined in the formula by the ".".
                            #ytest = NULL,       #<- This is already defined in the formula by "PREGNANT".
                            ntree = 500,        #<- Number of trees to grow. This should not be set to too small a number, to ensure that every input row gets classified at least a few times.
                            mtry = 5,            #<- Number of variables randomly sampled as candidates at each split. Default number for classification is sqrt(# of variables). Default number for regression is (# of variables / 3).
                            replace = TRUE,      #<- Should sampled data points be replaced.
                            #classwt = NULL,     #<- Priors of the classes. Use this if you want to specify what proportion of the data SHOULD be in each class. This is relevant if your sample data is not completely representative of the actual population 
                            #strata = NULL,      #<- Not necessary for our purpose here.
                            sampsize = 100,      #<- Size of sample to draw each time.
                            nodesize = 5,        #<- Minimum numbers of data points in terminal nodes.
                            #maxnodes = NULL,    #<- Limits the number of maximum splits. 
                            importance = TRUE,   #<- Should importance of predictors be assessed?
                            #localImp = FALSE,   #<- Should casewise importance measure be computed? (Setting this to TRUE will override importance.)
                            proximity = FALSE,    #<- Should a proximity measure between rows be calculated?
                            norm.votes = TRUE,   #<- If TRUE (default), the final result of votes are expressed as fractions. If FALSE, raw vote counts are returned (useful for combining results from different runs).
                            do.trace = TRUE,     #<- If set to TRUE, give a more verbose output as randomForest is run.
                            keep.forest = TRUE,  #<- If set to FALSE, the forest will not be retained in the output object. If xtest is given, defaults to FALSE.
                            keep.inbag = TRUE)   #<- Should an n by ntree matrix be returned that keeps track of which samples are in-bag in which trees? 
#So Looking at the confusion matrix of movies_RF, we can see that the class.error for the 0 class is pretty low, sitting at only around 5.10% The class.error for the 1 class is higher, sitting ar around 50.4%. the OOB estimate of error rate is around ~16%. I'm going to interpret some of this data and visualize it along with the error rates before changing some of the parameters in the random forest function to see how it affects my model.
#OOB estimate of  error rate: 29%
```
```{r RevRF Model I Peek}
movies_RF
```


### Evaluation
Using the training and tune datasets tune the model in consideration of the number
of trees, the number of variables to sample and the sample size that optimize the model
output. 
```{r RevRF Model II}
# This is how you can call up the criteria we set for the random forest:
movies_RF$call
# Call up the confusion matrix and check the accuracy of the model.
movies_RF$confusion
movies_RF_acc = sum(movies_RF$confusion[row(movies_RF$confusion) == 
                                                col(movies_RF$confusion)]) / 
  sum(movies_RF$confusion)
movies_RF_acc
# 0.99
# The "inbag" argument shows you which data point is included in which trees.
str(as.data.frame(movies_RF$inbag))
#View(as.data.frame(movies_RF$inbag))
inbag <- as.data.frame(movies_RF$inbag)
sum(inbag[,500])
dim(movies_RF$inbag)
str(as.data.frame(movies_RF$proximity)) 
#View(as.data.frame(movies_RF$proximity)) #blank
err.rate <- as.data.frame(movies_RF$err.rate)
#View(err.rate)
#### Visualize random forest results ####
# Let's visualize the results of the random forest.
# Let's start by looking at how the error rate changes as we add more trees.
movies_RF_error = data.frame(1:nrow(movies_RF$err.rate),
                                movies_RF$err.rate)
colnames(movies_RF_error) = c("Number of Trees", "Out of the Bag",
                                 "<$1M", ">$1M")
movies_RF_error$Diff <- movies_RF_error$`>$1M`-movies_RF_error$`<$1M`
#View(movies_RF_error)
library(plotly)
fig <- plot_ly(x=movies_RF_error$`Number of Trees`, y=movies_RF_error$Diff,name="Diff", type = 'scatter', mode = 'lines')
fig <- fig %>% add_trace(y=movies_RF_error$`Out of the Bag`, name="OOB_Er")
fig <- fig %>% add_trace(y=movies_RF_error$`<$1M`, name="<$1M")
fig <- fig %>% add_trace(y=movies_RF_error$`>$1M`, name=">$1M")
fig
```
## Conclusion
### Fairness Assessment
Our model does not use any protected classes, but the language of a movie could present some potential bias. Moviegoers might be less inclined to go see a movie in a different language than their own, and seeing as this dataset is in english, its safe to say that most of the voters spoke english but might not have spoken other languages. As such, this model should not be considered a globally reliable model.

### Summary 
In the end, all of our final models performed better than baseline, meaning that we could find some use for them in the movie industry. The average vote metric itself seemed to be more difficult to optimize for than revenue, which makes sense considering the budget parameter was strongly correlated to revenue but not to average vote. In the end, what makes a movie good or bad are the people behind it. A compelling performance can't be quantified so easily as budget, and a good script would take a lot longer to unpack than runtime. Regardless, the models we produced could still find some use in the movie industry to help directors and studios figure out which genre of movie they want to make, or how much money to invest in a particular project.

### Future Work
One thing I would be interested in looking into in the future is if we could do a sentiment analysis on movie scripts to determine the effect of general movie sentiment on success. I hypothesize that movies that are too dark or too upbeat would do worse critcially than balanced movies. It would also be fascinating to see if the genre of a movie could be predictive of different quantifications of the scripts such as word count, word complexity, and sentiment. Another future project could involve an analysis of the top movie studios, and whether data such as scripts and genres seems to cluster for each individual production company or studio. This type of information could help inform smaller studios how to become successful and give them good outlines for movie strucutre.